{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summit_Notebook_v0_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "7dd336b956e436c3e4239a1607ccc59572089974d552a2947c38e44483f0f2ff"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dbliZIia6dT"
      },
      "source": [
        "# Clustering with the 20-Newsgroups Dataset\n",
        "Authors: R. Edwards, J.Giles, N. Velzboer, E. Whitney and you! <br>\n",
        "Purpose: Exploring different clustering approaches with the 20 newsgroup dataset, comprising of user questions and posts on different web forums (open source: http://qwone.com/~jason/20Newsgroups/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcNxwP2Y6b4c"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQTKfZZca6dV"
      },
      "source": [
        "# Import Relavent Libraries\n",
        "\n",
        "## Python libraries \n",
        "import string\n",
        "import time\n",
        "\n",
        "## Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "## Plotting Data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Feature selection\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "## Dimensionality Reduction \n",
        "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "\n",
        "## Clustering \n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "## Get rid of pesky warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCx4HlrPa6dW"
      },
      "source": [
        "## Set-up\n",
        "Reading in data, creating pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ndqpukYa6dW"
      },
      "source": [
        "# Sklearn offers this data as part of their inbuilt learning sets.\n",
        "#  We'll download the 'training' set,  however ignore the name 'train'. They \n",
        "# offer the data split into training and testing sets if you are trying to apply some classification approach \n",
        "# May take up to a minute to download. We also filter the data for posts that are NSFW\n",
        "filter_topics = ['comp.graphics','comp.os.ms-windows.misc',\n",
        "'comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','comp.windows.x',\n",
        " 'misc.forsale','rec.autos','rec.motorcycles',\n",
        " 'rec.sport.baseball','rec.sport.hockey','sci.crypt',\n",
        " 'sci.electronics','sci.med','sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',categories=filter_topics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPcQ6m8ia6dW"
      },
      "source": [
        "# Pull data into pandas dataframe. Non essential, but pandas dataframes are nice to work with\n",
        "df_text_ng=pd.DataFrame(data={'text':newsgroups_train.data}, columns=['text'])\n",
        "\n",
        "# Take subset of dataset to prevent memory/kernel/processing issues\n",
        "row_lim = 2000\n",
        "df_text_ng = df_text_ng.iloc[0:row_lim,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYrQZLBYa6dX"
      },
      "source": [
        "## Cleaning and pre-processing\n",
        "We will apply the following pre-processing steps <br>\n",
        "<ol>\n",
        "<li> Remove unwanted characters\n",
        "<li> Tokenisation and case sensitive \n",
        "<li> Stopwords\n",
        "<li> Lemmitisation \n",
        "<ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2nxGTpZa6dX"
      },
      "source": [
        "# Standard NLTK stopwords list\n",
        "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
        "            \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
        "            'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
        "            'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
        "            'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
        "            'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n",
        "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
        "            'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
        "            'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', \n",
        "            'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
        "            'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', \n",
        "            'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', \n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', \n",
        "            'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
        "            'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \n",
        "            'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", \n",
        "            'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
        "            'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
        "            'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
        "            \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'would']\n",
        "# Add extras you want to remove here\n",
        "custom_stops = ['subject', 'from', 'to', 'article', 'summary','nntp', 'posting',\n",
        "                'host', 'lines', 'organisation', 'organization', 'scsi', 'writes',\n",
        "                'university', 'ax']\n",
        "# Join two lists\n",
        "our_stopwords = stopwords + custom_stops\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vspba67ta6dY"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 1000)\n",
        "# Lower case\n",
        "df_text_ng['text_lower'] = df_text_ng['text'].str.lower()\n",
        "# Remove \\n (newlines), and strings with emails or .com IP style addresses\n",
        "df_text_ng['text_noemails'] = df_text_ng['text_lower'].replace(r\"\\n\",\" \",\n",
        "                              regex=True).replace(r\"\\S*[@.]\\S*\\s?\",\"\", regex=True)\n",
        "# Remove anything except letters and spaces\n",
        "df_text_ng['text_punct'] = df_text_ng['text_noemails'].replace(r\"[^a-z ]\",\" \",\n",
        "                              regex=True)\n",
        "# Split string into lists of words on the whitespace\n",
        "df_text_ng['text_tokens'] = df_text_ng['text_punct'].str.split()\n",
        "# Remove words not in our prederived list\n",
        "df_text_ng['text_stop'] = df_text_ng['text_tokens'].apply(lambda x: [word \n",
        "                             for word in x if word not in our_stopwords])\n",
        "# Rejoin into a string \n",
        "df_text_ng['text_clean'] = df_text_ng['text_stop'].apply(lambda x: ' '.join([word for word in x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJxbMLeoa6dY"
      },
      "source": [
        "print(\"Original record 1: \\n\\n\")\n",
        "print(df_text_ng['text'][0:1])\n",
        "print(\"\\n\")\n",
        "print(\"Cleaned record 1: \\n\\n\")\n",
        "print(df_text_ng['text_clean'][0:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsMWcWbha6dZ"
      },
      "source": [
        "## Feature engineering\n",
        "Below we'll create the three possible sets that may be used (There are other <br>methods\n",
        "but these are the voting options) <br>\n",
        "<ol>\n",
        "<li> Set 1: Bag of words with Unigrams\n",
        "<li> Set 2: Bag of words with Unigrams and bigrams\n",
        "<li> Set 3: TF-IDF with unigrams\n",
        "</ol> <br>\n",
        "Uncomment the set you would like to create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z62HB97Va6dZ"
      },
      "source": [
        " #Uncomment this cell for Set 1\n",
        " bogS1 = CountVectorizer()\n",
        " X = bogS1.fit_transform(df_text_ng['text_clean'])\n",
        " print(X.toarray())\n",
        " print(\"First 20 features:....\\n\")\n",
        " print(bogS1.get_feature_names()[0:20])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqcJmW6Ma6dZ"
      },
      "source": [
        "#Uncomment this cell for Set 2\n",
        "bogS2 = CountVectorizer(ngram_range=(1,2))\n",
        "X = bogS2.fit_transform(df_text_ng['text_clean'])\n",
        "print(\"First 20 features:....\\n\")\n",
        "print(bogS2.get_feature_names()[0:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnWKbbQLa6da"
      },
      "source": [
        "# #Uncomment this cell for Set 3\n",
        "tfidfvect = TfidfVectorizer()\n",
        "X = tfidfvect.fit_transform(df_text_ng['text_clean'])\n",
        "print(\"First 20 features:....\\n\")\n",
        "print(tfidfvect.get_feature_names()[0:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4vpL3Pa6da"
      },
      "source": [
        "## Dimensionality reduction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cBpcqXfa6da"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAZRwFV_a6da"
      },
      "source": [
        "n_components = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGMov82ra6db"
      },
      "source": [
        "# LDA\n",
        "print(\"Performing dimensionality reduction using LDA...\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=n_components, random_state=0, evaluate_every=1)\n",
        "lda_model = lda.fit(X)\n",
        "X_DR = lda.fit_transform(X)\n",
        "\n",
        "end = time.time()\n",
        "duration = np.round(end - start)\n",
        "print(\"LDA duration: %i s\" %(duration))\n",
        "# plt.plot(X_DR.max(axis=1),'o')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scpR2xhIa6db"
      },
      "source": [
        "### LSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxwiZeaPa6db"
      },
      "source": [
        "# LSA\n",
        "## Prepare data for clustering\n",
        "\n",
        "print(\"Performing dimensionality reduction using LSA...\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# # Vectorizer results are normalized, which makes KMeans behave as\n",
        "# # spherical k-means for better results. Since LSA/SVD results are\n",
        "# # not normalized, we have to redo the normalization.\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "X_array = X.toarray()\n",
        "X_DR = lsa.fit_transform(X)\n",
        "\n",
        "end = time.time()\n",
        "duration = np.round(end - start)\n",
        "print(\"LSA duration: %i s\" %(duration))\n",
        "\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(\"Explained variance of the SVD step: {}%\".format(\n",
        "     int(explained_variance * 100)))\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(svd.n_components)\n",
        "\n",
        "plt.bar(features, svd.explained_variance_ratio_, color='black')\n",
        "plt.xlabel('LSA features')\n",
        "plt.ylabel('variance %');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl5XDsZKa6dc"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSOQPyd2a6dc"
      },
      "source": [
        "# PCA\n",
        "\n",
        "## Prepare data for clustering\n",
        "\n",
        "print(\"Performing dimensionality reduction using PCA...\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_DR = pca.fit_transform(X.toarray())\n",
        "principalDf = pd.DataFrame(data = X_DR)\n",
        "\n",
        "print(\"Explained variance of the PCA: {}%\".format(\n",
        "     int(pca.explained_variance_ratio_.sum() * 100)))\n",
        "\n",
        "end = time.time()\n",
        "duration = np.round(end - start)\n",
        "print(\"LDA duration: %i s\" %(duration))\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
        "plt.xlabel('PCA features')\n",
        "plt.ylabel('variance %');\n",
        "print('The shape of X_DR is: ', X_DR.shape)\n",
        "#plt.xticks(features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQRA210Qa6dc"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHyh6tjSa6dc"
      },
      "source": [
        "### Kmeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRq0n2uCa6dd"
      },
      "source": [
        "# Initialise Modelling Parameters\n",
        "\n",
        "## Number of CLusters\n",
        "K = 14\n",
        "\n",
        "## Method for Initialisation\n",
        "init='k-means++' ## k-means++ selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n",
        "\n",
        "## The number of initializations to perform. The best results are kept.\n",
        "n_init=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTag-3d9a6dd"
      },
      "source": [
        "## Create model instance\n",
        "model_instance = KMeans(n_clusters = K,\n",
        "                        init=init,\n",
        "                        n_init=n_init\n",
        "                       )\n",
        "\n",
        "## Fit data to model\n",
        "model = model_instance.fit(X_DR)\n",
        "\n",
        "## Get labels for evaluation\n",
        "labels = model.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkEEPdwla6dd"
      },
      "source": [
        "### Gaussian Mixture Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqlBjt_Ta6dd"
      },
      "source": [
        "# Initialise Modelling Parameters\n",
        "\n",
        "## Number of CLusters\n",
        "K = 14\n",
        "\n",
        "## String describing the type of covariance parameters to use.\n",
        "covariance_type='full' ## Full means each component has its own general covariance matrix\n",
        "\n",
        "## The number of initializations to perform. The best results are kept.\n",
        "n_init=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gsBlJEka6dd"
      },
      "source": [
        "## Create model instance\n",
        "model_instance = GaussianMixture(n_components=K\n",
        "                                , n_init=n_init\n",
        "                                , covariance_type=covariance_type\n",
        "                                )\n",
        "\n",
        "## Fit data to model\n",
        "model = model_instance.fit(X_DR)\n",
        "\n",
        "## Get labels for evaluation\n",
        "labels = model.predict(X_DR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URjsXD0Ta6de"
      },
      "source": [
        "### DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVsfFoL7a6de"
      },
      "source": [
        "# Initialise Modelling Parameters\n",
        "\n",
        "## The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
        "eps = 0.5\n",
        "\n",
        "## The number of samples in a neighborhood for a point to be considered as a core point. This includes the point itself.\n",
        "min_samples = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlRZOoiga6de"
      },
      "source": [
        "## Create model instance\n",
        "model_instance = DBSCAN(eps=eps\n",
        "                        , min_samples=min_samples\n",
        "                        )\n",
        "\n",
        "## Fit data to model\n",
        "model = model_instance.fit(X_DR)\n",
        "\n",
        "## Get labels for evaluation\n",
        "labels = model.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwW_2YCha6de"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSv28dP2a6de"
      },
      "source": [
        "# Silhouette Score\n",
        "silhouette_score = metrics.silhouette_score(X_DR\n",
        "                                            , labels\n",
        "                                           )\n",
        "print(\"Silhouette Score:\\t\" + str(silhouette_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuOwHQora6de"
      },
      "source": [
        "# Davies-Bouldin Score\n",
        "davies_bouldin_score = metrics.davies_bouldin_score(X_DR\n",
        "                                                    , labels\n",
        "                                                   )\n",
        "\n",
        "print(\"Davies-Bouldin Score:\\t\" + str(davies_bouldin_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QzInXXVa6df"
      },
      "source": [
        "# Coefficient of Variation\n",
        "## Assign Data points with their clusters\n",
        "df_text_ng['cluster'] = labels\n",
        "df_text_ng['cluster'].value_counts().plot(kind='bar', figsize=(15,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ha1QzWa6df"
      },
      "source": [
        "# Top Frequency Words per Cluster\n",
        "df_text_ng[['text_clean', 'cluster']]\n",
        "\n",
        "# Get distinct clusters ordered from highest frequency to lowest frequecy\n",
        "clusters = np.sort(df_text_ng[\"cluster\"].unique())\n",
        "\n",
        "# Number of top words to print\n",
        "print_top_x_words = 5\n",
        "\n",
        "# for each cluster\n",
        "for i in range (0, len(clusters)):\n",
        "    \n",
        "    # Print Cluster Header\n",
        "    cluster = clusters[i]\n",
        "    num_terms = df_text_ng['cluster'].value_counts().tolist()[i]\n",
        "    print(\"Cluster: \" + str(cluster) + \"\\t Number of Terms: \" + str(num_terms))\n",
        "    \n",
        "    # Print Frequency Counts\n",
        "    vectorizer = CountVectorizer().fit(df_text_ng[df_text_ng['cluster']==cluster]['text_clean'].values)\n",
        "    cluster_bog = vectorizer.transform(df_text_ng[df_text_ng['cluster']==cluster]['text_clean'].values)\n",
        "    word_sum = cluster_bog.sum(axis=0)\n",
        "    words_freq = [(word, word_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "    words_freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    print_str = \"\"\n",
        "    for item in words_freq_sorted[:print_top_x_words]:\n",
        "        print_str += (str(item[0]) + \" (\" + str(item[1]) + \")\\t\")\n",
        "    print(print_str + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282gTXXDKfAJ"
      },
      "source": [
        "3 examples from each cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5P4QKDNpoPV"
      },
      "source": [
        "for k in range(1, len(df_text_ng[\"cluster\"].unique())):\n",
        "    for i in range(0,3):\n",
        "        print('Cluster: %i\\t Forum post:\\t %i\\n-----------------------------------\\n' %(k,i))\n",
        "        # Print Cluster Header\n",
        "        # Print Frequency Counts\n",
        "        cluster = clusters[k]\n",
        "        vectorizer = CountVectorizer().fit(df_text_ng[df_text_ng['cluster']==cluster]['text_clean'].values)\n",
        "        cluster_bog = vectorizer.transform(df_text_ng[df_text_ng['cluster']==cluster]['text_clean'].values)\n",
        "        word_sum = cluster_bog.sum(axis=0)\n",
        "        words_freq = [(word, word_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "        words_freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "        print_str = \"\"\n",
        "        for item in words_freq_sorted[:print_top_x_words]:\n",
        "            print_str += (str(item[0]) + \" (\" + str(item[1]) + \")\\t\")\n",
        "        num_terms = df_text_ng['cluster'].value_counts().tolist()[k]            \n",
        "        print(print_str + \"\\n\")\n",
        "        print(df_text_ng[\"text\"][df_text_ng[\"cluster\"]==k].iloc[i])\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPs62Uyq6b4s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}